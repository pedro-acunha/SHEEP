{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3043aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/envs/rapids/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/envs/rapids/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: flaml in /opt/conda/envs/rapids/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from flaml) (1.0.1)\n",
      "Requirement already satisfied: xgboost<=1.3.3,>=0.90 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from flaml) (1.3.3)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from flaml) (1.2.4)\n",
      "Requirement already satisfied: lightgbm>=2.3.1 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from flaml) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from flaml) (1.6.0)\n",
      "Requirement already satisfied: NumPy>=1.16.2 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from flaml) (1.20.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/rapids/lib/python3.7/site-packages (from lightgbm>=2.3.1->flaml) (0.36.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from pandas>=1.1.4->flaml) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from pandas>=1.1.4->flaml) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->flaml) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from scikit-learn>=0.24->flaml) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from scikit-learn>=0.24->flaml) (2.1.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/envs/rapids/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/envs/rapids/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/envs/rapids/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/envs/rapids/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install scikit-learn-intelex\n",
    "!pip install flaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412e5cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  2 11:55:03 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 4000     Off  | 00000000:00:05.0 Off |                  N/A |\n",
      "| 30%   26C    P8     5W / 125W |      0MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3545090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "from sklearn.metrics import r2_score,accuracy_score, confusion_matrix, classification_report,precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from flaml import AutoML\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49da9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colours(data, features):\n",
    "    \"\"\"Create dataframe with colour-colour data. Return an Array and list of colours.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Dataframe with photometric data\n",
    "        features (list): List of features from photometric data\n",
    "    \"\"\"\n",
    "    N = len(data)\n",
    "    F = len(features)\n",
    "    n=0\n",
    "    for i in np.linspace(1,F,F,dtype=int):\n",
    "    \tn = n + (i-1)\n",
    "\n",
    "    df_features = np.zeros((N, n))\n",
    "    y=0\n",
    "    lista=[]\n",
    "    for z in np.linspace(0,F,F,dtype=int):\n",
    "        for x in np.linspace(1,F-1,F-1,dtype=int):\n",
    "            if z!=x and z<x:\n",
    "                df_features[:,y] = np.abs(data[features[z]] - data[features[x]])\n",
    "                y+=1\n",
    "                lista += [features[z]+'-'+features[x]]\n",
    "            else:\n",
    "                pass\n",
    "    df_colours = pd.DataFrame(df_features,columns = lista)\n",
    "    return df_colours\n",
    "\n",
    "def create_colours_filter(data, features):\n",
    "    # Function to create colours by broadband magnitude filter\n",
    "    N = len(data)\n",
    "    F = len(features)\n",
    "        \n",
    "    df_features = np.zeros((N, F-2))\n",
    "    y=0\n",
    "    lista=[]\n",
    "    for z in np.linspace(0,F-1,F-1,dtype=int):\n",
    "        for x in np.linspace(1,F-1,F-1,dtype=int):\n",
    "            if (features[z]!=features[x]) and (features[z][-2:] == features[x][-2:]):\n",
    "                df_features[:,y] = np.abs(data[features[z]] - data[features[x]])\n",
    "                y+=1\n",
    "                lista += [features[z]+'-'+features[x]]\n",
    "            else:\n",
    "                pass\n",
    "    df_colours = pd.DataFrame(df_features,columns = lista)\n",
    "    \n",
    "    return df_colours    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e647775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_opt_model(model,X_train, y_train, y_test):\n",
    "    # model (sklearn, xgb, etc): model to optimize\n",
    "    # X_train (DataFrame): Data for training\n",
    "    # y_train (Series): Data to predict. IMPORTANT: Specify the variable to target\n",
    "    params = {'n_estimators': [1000,2000],\n",
    "          'max_depth': [0, 12],\n",
    "         'learning_rate': [0.1,1]}\n",
    "\n",
    "    opt = BayesSearchCV(\n",
    "        model,\n",
    "        params,\n",
    "        n_iter=5,\n",
    "        cv=3\n",
    "    )\n",
    "\n",
    "    opt.fit(X_train, y_train)\n",
    "\n",
    "    print(\"val. score: %s\" % opt.best_score_)\n",
    "    print(\"test score: %s\" % opt.score(X_test, y_test))\n",
    "    \n",
    "    return opt.best_estimator_\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "def kf_oof(clf,X_train,y_train,X_test, var,nkf):\n",
    "\n",
    "    kf= RepeatedKFold(n_splits=5,random_state=42)\n",
    "    kf.get_n_splits(features)\n",
    "\n",
    "    oof_predictions = np.zeros(len(X_train))\n",
    "    oof_test_predictions = np.zeros(len(X_test))\n",
    "    i=0\n",
    "    \n",
    "    for train_index, holdout_index in kf.split(X_train, y_train[var]):\n",
    "        i+=1\n",
    "\n",
    "        clf.fit(X_train.iloc[train_index], y_train[var].iloc[train_index])\n",
    "    \n",
    "        y_pred = clf.predict(X_train.iloc[holdout_index])\n",
    "        oof_predictions[holdout_index] = y_pred\n",
    "    \n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        oof_test_predictions += y_test_pred\n",
    "        print(\"For cycle loop:\",i)\n",
    "        \n",
    "\n",
    "    oof_test_predictions = oof_test_predictions/i\n",
    "    return oof_predictions, oof_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30a4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cat_outliers(x, var1, var2):\n",
    "    met = np.abs(pd.Series(x[var2]-x[var1]))\n",
    "    f_out = met/(1+x[var1].astype(np.float32))\n",
    "    y_outlier = pd.Series(np.where(f_out > 0.15, 'outlier', 'not outlier')).reindex(x.index)\n",
    "    #df = x.drop(y_outlier[y_outlier=='outlier'].index.to_list())\n",
    "    #df_nout = x.drop(y_outlier[y_outlier=='not outlier'].index.to_list())\n",
    "    print(\"Outliers: \\n\", y_outlier.value_counts())\n",
    "    return y_outlier\n",
    "\n",
    "def flaml_opt(estimator, X_train, y_train):\n",
    "    model = AutoML()\n",
    "    \n",
    "    if estimator == ['xgb']:\n",
    "        \n",
    "        settings = {\n",
    "        \"time_budget\": 4200,  # total running time in seconds\n",
    "        \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2']\n",
    "        \"estimator_list\": ['xgboost'],  # list of ML learners; we tune xgboost in this example\n",
    "        \"n_jobs\": -1,\n",
    "        \"task\": 'classification',  # task type    \n",
    "        \"log_file_name\": 'clf.log'  # flaml log file\n",
    "        }\n",
    "    \n",
    "    \n",
    "    elif estimator == ['cb']:\n",
    "        \n",
    "        settings = {\n",
    "        \"time_budget\": 4200,  # total running time in seconds\n",
    "        \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2']\n",
    "        \"estimator_list\": ['xgboost'],  # list of ML learners; we tune xgboost in this example\n",
    "        \"n_jobs\": -1,\n",
    "        \"tree_method\":'gpu_hist',\n",
    "        \"task\": 'classification',  # task type    \n",
    "        \"log_file_name\": 'clf.log'  # flaml log file\n",
    "        }\n",
    "        \n",
    "    elif estimator == ['lgb']:\n",
    "        \n",
    "        settings = {\n",
    "        \"time_budget\": 4200,  # total running time in seconds\n",
    "        \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2']\n",
    "        \"estimator_list\": ['lgbm'],  # list of ML learners; we tune xgboost in this example\n",
    "        \"n_jobs\": -1,\n",
    "        \"task\": 'classification',  # task type    \n",
    "        \"log_file_name\": 'clf.log'  # flaml log file\n",
    "        }\n",
    "    elif estimator == ['all']:\n",
    "        settings = {\n",
    "        \"time_budget\": 4200,  # total running time in seconds\n",
    "        \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2']\n",
    "        \"n_jobs\": -1,\n",
    "        \"task\": 'classification',  # task type    \n",
    "        \"log_file_name\": 'clf.log'  # flaml log file\n",
    "        }\n",
    "        \n",
    "        \n",
    "    model.fit(X_train=X_train, y_train=y_train, **settings)\n",
    "\n",
    "    print('Best estimator:', model.best_config_per_estimator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072c02f",
   "metadata": {},
   "source": [
    "# Feature Engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9fd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV from DR15 SDSS\n",
    "\n",
    "df_psf = cudf.read_csv('./Data/SDSS_DR15_zspec.csv')\n",
    "list_features = ['specObjID','psfMag_u', 'psfMag_g','psfMag_r','psfMag_i','psfMag_z',\n",
    "                 'w1mpro','w2mpro','w3mpro','w4mpro', 'z','class']\n",
    "df_psf = df_psf[list_features]\n",
    "df_psf['z'] = df_psf['z']+1\n",
    "\n",
    "# Read modelMag magnitudes\n",
    "df_mag = cudf.read_csv('./Data/SDSS_DR15_mag_PCunha.csv')\n",
    "list_mag = ['specObjID','modelMag_u', 'modelMag_g',\n",
    "       'modelMag_r', 'modelMag_i', 'modelMag_z']\n",
    "df_model = df_mag[list_mag]\n",
    "\n",
    "# Read modelMag magnitudes\n",
    "df_mag2 = cudf.read_csv('./Data/SDSS_DR15_add_photo_PCunha.csv')\n",
    "list_mag2 = ['specObjID','petroMag_u', 'petroMag_g', 'petroMag_r', 'petroMag_i',\n",
    "       'petroMag_z', 'cModelMag_u', 'cModelMag_g', 'cModelMag_r',\n",
    "       'cModelMag_i', 'cModelMag_z']\n",
    "df_model2 = df_mag2[list_mag2]\n",
    "\n",
    "# Read 2MASS and FIRST data\n",
    "#df_add = pd.read_csv('./Data/SDSS_DR15_phot_2MASS_first_PCunha.csv')\n",
    "#list_add = ['specObjID','j', 'h', 'k','first_peak', 'first_integrated']\n",
    "#df_add = df_add[list_add]\n",
    "\n",
    "#df = pd.merge(df_psf, df_add, on='specObjID', how='outer')\n",
    "df = cudf.merge(df_psf,df_model,on='specObjID', how='inner')\n",
    "df = cudf.merge(df,df_model2,on='specObjID', how='inner')\n",
    "\n",
    "# Read coords data\n",
    "df_coords = cudf.read_csv('./Data/SDSS_DR15_coords_PCunha.csv')\n",
    "\n",
    "df = cudf.merge(df,df_coords,on='specObjID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d04284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd806b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df_psf)\n",
    "del(df_model)\n",
    "del(df_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dcadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "\n",
    "df = df.drop_duplicates(subset='specObjID', keep=\"last\")\n",
    "\n",
    "# Convert to Pandas due to colour functions\n",
    "df = df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11120cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sdss2 = ['cModelMag_u', 'cModelMag_g','cModelMag_r', 'cModelMag_i', 'cModelMag_z']\n",
    "\n",
    "l_sdss3 = ['modelMag_u', 'modelMag_g', 'modelMag_r', 'modelMag_i', 'modelMag_z']\n",
    "\n",
    "l_sdss4 = ['psfMag_u', 'psfMag_g','psfMag_r','psfMag_i','psfMag_z',\n",
    "         'cModelMag_u', 'cModelMag_g','cModelMag_r', 'cModelMag_i', 'cModelMag_z']\n",
    "\n",
    "l_sdss5 = ['psfMag_u', 'psfMag_g','psfMag_r','psfMag_i','psfMag_z',\n",
    "           'modelMag_u', 'modelMag_g', 'modelMag_r', 'modelMag_i', 'modelMag_z']\n",
    "\n",
    "l_sdss_wise = ['psfMag_u', 'psfMag_g','psfMag_r','psfMag_i','psfMag_z',\n",
    "         'w1mpro','w2mpro','w3mpro','w4mpro']\n",
    "\n",
    "\n",
    "df_colours_sdss2 = create_colours(df, l_sdss2)\n",
    "df_colours_sdss3 = create_colours(df, l_sdss3)\n",
    "df_colours_sdss4 = create_colours_filter(df, l_sdss4)\n",
    "df_colours_sdss5 = create_colours_filter(df, l_sdss5)\n",
    "df_colours_sdss_wise = create_colours(df, l_sdss_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f0de8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df.reset_index(drop=True),df_colours_sdss2.reset_index(drop=True)], axis=1, sort=False)\n",
    "df = pd.concat([df.reset_index(drop=True),df_colours_sdss3.reset_index(drop=True)], axis=1, sort=False)\n",
    "df = pd.concat([df.reset_index(drop=True),df_colours_sdss4.reset_index(drop=True)], axis=1, sort=False)\n",
    "df = pd.concat([df.reset_index(drop=True),df_colours_sdss5.reset_index(drop=True)], axis=1, sort=False)\n",
    "df = pd.concat([df.reset_index(drop=True),df_colours_sdss_wise.reset_index(drop=True)], axis=1, sort=False)\n",
    "\n",
    "del(df_colours_sdss2)\n",
    "del(df_colours_sdss3)\n",
    "del(df_colours_sdss4)\n",
    "del(df_colours_sdss5)\n",
    "del(df_colours_sdss_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7875f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w1-w2-w3'] = np.abs(df['w1mpro']-df['w2mpro']-df['w3mpro'])\n",
    "df['w1-w2-w3-w4'] = np.abs(df['w1mpro']-df['w2mpro']-df['w3mpro']-df['w4mpro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a656d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['specObjID'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329b5d2",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636c95b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = df.columns.to_list()\n",
    "features.remove('class')\n",
    "features.remove('z')\n",
    "features.remove('specObjID')\n",
    "features.remove('ra')\n",
    "features.remove('dec')\n",
    "for i in range(len(features)):\n",
    "    print('Feature '+str(features[i])+': ',df[features[i]].loc[df[features[i]]<0].value_counts(dropna=False))\n",
    "    print('Max value :',df[features[i]].max())\n",
    "    print('Min value :',df[features[i]].min(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features)):\n",
    "    df[features[i]].loc[df[features[i]] > 50] = -9999.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed78fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(features)):\n",
    "    print('Feature '+str(features[i])+': ',df[features[i]].loc[df[features[i]]<0].value_counts(dropna=False))\n",
    "    print('Max value :',df[features[i]].max())\n",
    "    print('Min value :',df[features[i]].min(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9462d60",
   "metadata": {},
   "source": [
    "# Data scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class convertion into 0, 1, 2\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['class'])\n",
    "print('Labels [0,1,2]: ',le.inverse_transform([0,1,2]))\n",
    "print('Data encoded.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features)):\n",
    "    df[features[i]] = pd.to_numeric(df[features[i]], downcast=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f87353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7cf7be",
   "metadata": {},
   "source": [
    "# Export data for photo_z predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f19d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"./Data/SDSS_DR15_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea11e7b",
   "metadata": {},
   "source": [
    "# Import photo_z predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d972068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('./Data/SDSS_DR15_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63a6ab",
   "metadata": {},
   "source": [
    "# Add predicted photo_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae1ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add photometric redshifts predictions\n",
    "#df_meta_z2 = pd.read_csv('./Data/oof_z_spec_pred_v2.csv')\n",
    "#df_meta_z2.columns=['index','meta_z2']\n",
    "#df_meta_z2.set_index('index', inplace=True)\n",
    "#df = pd.merge(df,df_meta_z2, left_index=True, right_index=True)\n",
    "\n",
    "#del(df_meta_z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4499288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp_z = pd.read_csv('./Data/oof_z_pred_v2.csv')\n",
    "df_imp_z.columns=['index','imp_z']\n",
    "df_imp_z.set_index('index', inplace=True)\n",
    "df = pd.merge(df,df_imp_z, left_index=True, right_index=True)\n",
    "\n",
    "del(df_imp_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2bb507",
   "metadata": {},
   "source": [
    "# Add predicted catastrophic outlier in photo_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b4779f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_cat_outliers(x, var1, var2):\n",
    "    met = np.abs(pd.Series(x[var2]-x[var1]))\n",
    "    f_out = met/(1+x[var1].astype(np.float32))\n",
    "    y_outlier = pd.Series(np.where(f_out > 0.15, 'outlier', 'not outlier')).reindex(x.index)\n",
    "    x['flag_outlier'] = pd.Series(np.where(f_out > 0.15, 0, 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231e56df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specObjID</th>\n",
       "      <th>psfMag_u</th>\n",
       "      <th>psfMag_g</th>\n",
       "      <th>psfMag_r</th>\n",
       "      <th>psfMag_i</th>\n",
       "      <th>psfMag_z</th>\n",
       "      <th>w1mpro</th>\n",
       "      <th>w2mpro</th>\n",
       "      <th>w3mpro</th>\n",
       "      <th>w4mpro</th>\n",
       "      <th>...</th>\n",
       "      <th>psfMag_z-w4mpro</th>\n",
       "      <th>w1mpro-w2mpro</th>\n",
       "      <th>w1mpro-w3mpro</th>\n",
       "      <th>w1mpro-w4mpro</th>\n",
       "      <th>w2mpro-w3mpro</th>\n",
       "      <th>w2mpro-w4mpro</th>\n",
       "      <th>w3mpro-w4mpro</th>\n",
       "      <th>w1-w2-w3</th>\n",
       "      <th>w1-w2-w3-w4</th>\n",
       "      <th>imp_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299489676975171584</td>\n",
       "      <td>19.086210</td>\n",
       "      <td>17.417789</td>\n",
       "      <td>16.595810</td>\n",
       "      <td>16.034969</td>\n",
       "      <td>15.680770</td>\n",
       "      <td>11.856000</td>\n",
       "      <td>11.872</td>\n",
       "      <td>9.154</td>\n",
       "      <td>7.836</td>\n",
       "      <td>...</td>\n",
       "      <td>7.84477</td>\n",
       "      <td>0.016</td>\n",
       "      <td>2.702</td>\n",
       "      <td>4.020</td>\n",
       "      <td>2.718</td>\n",
       "      <td>4.036</td>\n",
       "      <td>1.318</td>\n",
       "      <td>9.170</td>\n",
       "      <td>17.006001</td>\n",
       "      <td>1.031694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>299489677444933632</td>\n",
       "      <td>19.086210</td>\n",
       "      <td>17.417789</td>\n",
       "      <td>16.595810</td>\n",
       "      <td>16.034969</td>\n",
       "      <td>15.680770</td>\n",
       "      <td>11.856000</td>\n",
       "      <td>11.872</td>\n",
       "      <td>9.154</td>\n",
       "      <td>7.836</td>\n",
       "      <td>...</td>\n",
       "      <td>7.84477</td>\n",
       "      <td>0.016</td>\n",
       "      <td>2.702</td>\n",
       "      <td>4.020</td>\n",
       "      <td>2.718</td>\n",
       "      <td>4.036</td>\n",
       "      <td>1.318</td>\n",
       "      <td>9.170</td>\n",
       "      <td>17.006001</td>\n",
       "      <td>1.033450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>299489951853078528</td>\n",
       "      <td>22.678499</td>\n",
       "      <td>20.570419</td>\n",
       "      <td>19.164761</td>\n",
       "      <td>18.745230</td>\n",
       "      <td>18.312450</td>\n",
       "      <td>14.574000</td>\n",
       "      <td>14.207</td>\n",
       "      <td>11.902</td>\n",
       "      <td>8.626</td>\n",
       "      <td>...</td>\n",
       "      <td>9.68645</td>\n",
       "      <td>0.367</td>\n",
       "      <td>2.672</td>\n",
       "      <td>5.948</td>\n",
       "      <td>2.305</td>\n",
       "      <td>5.581</td>\n",
       "      <td>3.276</td>\n",
       "      <td>11.535</td>\n",
       "      <td>20.160999</td>\n",
       "      <td>1.210131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299489952322840576</td>\n",
       "      <td>22.343210</td>\n",
       "      <td>20.316629</td>\n",
       "      <td>18.933720</td>\n",
       "      <td>18.372570</td>\n",
       "      <td>17.988461</td>\n",
       "      <td>14.226000</td>\n",
       "      <td>13.918</td>\n",
       "      <td>12.521</td>\n",
       "      <td>9.106</td>\n",
       "      <td>...</td>\n",
       "      <td>8.88246</td>\n",
       "      <td>0.308</td>\n",
       "      <td>1.705</td>\n",
       "      <td>5.120</td>\n",
       "      <td>1.397</td>\n",
       "      <td>4.812</td>\n",
       "      <td>3.415</td>\n",
       "      <td>12.213</td>\n",
       "      <td>21.319000</td>\n",
       "      <td>1.213816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>299490226730985472</td>\n",
       "      <td>20.024401</td>\n",
       "      <td>19.601589</td>\n",
       "      <td>19.492901</td>\n",
       "      <td>19.371880</td>\n",
       "      <td>19.298290</td>\n",
       "      <td>15.102000</td>\n",
       "      <td>14.071</td>\n",
       "      <td>11.880</td>\n",
       "      <td>8.372</td>\n",
       "      <td>...</td>\n",
       "      <td>10.92629</td>\n",
       "      <td>1.031</td>\n",
       "      <td>3.222</td>\n",
       "      <td>6.730</td>\n",
       "      <td>2.191</td>\n",
       "      <td>5.699</td>\n",
       "      <td>3.508</td>\n",
       "      <td>10.849</td>\n",
       "      <td>19.221001</td>\n",
       "      <td>1.659689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474913</th>\n",
       "      <td>11259270496166846464</td>\n",
       "      <td>23.912531</td>\n",
       "      <td>23.555460</td>\n",
       "      <td>22.746799</td>\n",
       "      <td>21.878040</td>\n",
       "      <td>20.049231</td>\n",
       "      <td>15.729000</td>\n",
       "      <td>15.002</td>\n",
       "      <td>11.273</td>\n",
       "      <td>8.584</td>\n",
       "      <td>...</td>\n",
       "      <td>11.46523</td>\n",
       "      <td>0.727</td>\n",
       "      <td>4.456</td>\n",
       "      <td>7.145</td>\n",
       "      <td>3.729</td>\n",
       "      <td>6.418</td>\n",
       "      <td>2.689</td>\n",
       "      <td>10.546</td>\n",
       "      <td>19.129999</td>\n",
       "      <td>2.044420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474914</th>\n",
       "      <td>11259270771044753408</td>\n",
       "      <td>19.719530</td>\n",
       "      <td>19.538170</td>\n",
       "      <td>19.356890</td>\n",
       "      <td>18.952471</td>\n",
       "      <td>18.992161</td>\n",
       "      <td>15.687000</td>\n",
       "      <td>14.676</td>\n",
       "      <td>11.301</td>\n",
       "      <td>8.737</td>\n",
       "      <td>...</td>\n",
       "      <td>10.25516</td>\n",
       "      <td>1.011</td>\n",
       "      <td>4.386</td>\n",
       "      <td>6.950</td>\n",
       "      <td>3.375</td>\n",
       "      <td>5.939</td>\n",
       "      <td>2.564</td>\n",
       "      <td>10.290</td>\n",
       "      <td>19.027000</td>\n",
       "      <td>2.857131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474915</th>\n",
       "      <td>11259271045922660352</td>\n",
       "      <td>26.007179</td>\n",
       "      <td>23.568859</td>\n",
       "      <td>22.621071</td>\n",
       "      <td>21.113041</td>\n",
       "      <td>20.064899</td>\n",
       "      <td>16.054001</td>\n",
       "      <td>15.412</td>\n",
       "      <td>11.976</td>\n",
       "      <td>7.921</td>\n",
       "      <td>...</td>\n",
       "      <td>12.14390</td>\n",
       "      <td>0.642</td>\n",
       "      <td>4.078</td>\n",
       "      <td>8.133</td>\n",
       "      <td>3.436</td>\n",
       "      <td>7.491</td>\n",
       "      <td>4.055</td>\n",
       "      <td>11.334</td>\n",
       "      <td>19.254999</td>\n",
       "      <td>1.938109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474916</th>\n",
       "      <td>11259271320800567296</td>\n",
       "      <td>24.128031</td>\n",
       "      <td>22.385111</td>\n",
       "      <td>21.015579</td>\n",
       "      <td>20.358210</td>\n",
       "      <td>19.912359</td>\n",
       "      <td>16.104000</td>\n",
       "      <td>15.665</td>\n",
       "      <td>12.139</td>\n",
       "      <td>8.793</td>\n",
       "      <td>...</td>\n",
       "      <td>11.11936</td>\n",
       "      <td>0.439</td>\n",
       "      <td>3.965</td>\n",
       "      <td>7.311</td>\n",
       "      <td>3.526</td>\n",
       "      <td>6.872</td>\n",
       "      <td>3.346</td>\n",
       "      <td>11.700</td>\n",
       "      <td>20.493000</td>\n",
       "      <td>1.417181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474917</th>\n",
       "      <td>11259274069579636736</td>\n",
       "      <td>23.662201</td>\n",
       "      <td>22.239361</td>\n",
       "      <td>20.752819</td>\n",
       "      <td>19.511320</td>\n",
       "      <td>18.673740</td>\n",
       "      <td>16.666000</td>\n",
       "      <td>16.343</td>\n",
       "      <td>12.111</td>\n",
       "      <td>8.795</td>\n",
       "      <td>...</td>\n",
       "      <td>9.87874</td>\n",
       "      <td>0.323</td>\n",
       "      <td>4.555</td>\n",
       "      <td>7.871</td>\n",
       "      <td>4.232</td>\n",
       "      <td>7.548</td>\n",
       "      <td>3.316</td>\n",
       "      <td>11.788</td>\n",
       "      <td>20.583000</td>\n",
       "      <td>1.000804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3474918 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    specObjID   psfMag_u   psfMag_g   psfMag_r   psfMag_i  \\\n",
       "0          299489676975171584  19.086210  17.417789  16.595810  16.034969   \n",
       "1          299489677444933632  19.086210  17.417789  16.595810  16.034969   \n",
       "2          299489951853078528  22.678499  20.570419  19.164761  18.745230   \n",
       "3          299489952322840576  22.343210  20.316629  18.933720  18.372570   \n",
       "4          299490226730985472  20.024401  19.601589  19.492901  19.371880   \n",
       "...                       ...        ...        ...        ...        ...   \n",
       "3474913  11259270496166846464  23.912531  23.555460  22.746799  21.878040   \n",
       "3474914  11259270771044753408  19.719530  19.538170  19.356890  18.952471   \n",
       "3474915  11259271045922660352  26.007179  23.568859  22.621071  21.113041   \n",
       "3474916  11259271320800567296  24.128031  22.385111  21.015579  20.358210   \n",
       "3474917  11259274069579636736  23.662201  22.239361  20.752819  19.511320   \n",
       "\n",
       "          psfMag_z     w1mpro  w2mpro  w3mpro  w4mpro  ...  psfMag_z-w4mpro  \\\n",
       "0        15.680770  11.856000  11.872   9.154   7.836  ...          7.84477   \n",
       "1        15.680770  11.856000  11.872   9.154   7.836  ...          7.84477   \n",
       "2        18.312450  14.574000  14.207  11.902   8.626  ...          9.68645   \n",
       "3        17.988461  14.226000  13.918  12.521   9.106  ...          8.88246   \n",
       "4        19.298290  15.102000  14.071  11.880   8.372  ...         10.92629   \n",
       "...            ...        ...     ...     ...     ...  ...              ...   \n",
       "3474913  20.049231  15.729000  15.002  11.273   8.584  ...         11.46523   \n",
       "3474914  18.992161  15.687000  14.676  11.301   8.737  ...         10.25516   \n",
       "3474915  20.064899  16.054001  15.412  11.976   7.921  ...         12.14390   \n",
       "3474916  19.912359  16.104000  15.665  12.139   8.793  ...         11.11936   \n",
       "3474917  18.673740  16.666000  16.343  12.111   8.795  ...          9.87874   \n",
       "\n",
       "         w1mpro-w2mpro  w1mpro-w3mpro  w1mpro-w4mpro  w2mpro-w3mpro  \\\n",
       "0                0.016          2.702          4.020          2.718   \n",
       "1                0.016          2.702          4.020          2.718   \n",
       "2                0.367          2.672          5.948          2.305   \n",
       "3                0.308          1.705          5.120          1.397   \n",
       "4                1.031          3.222          6.730          2.191   \n",
       "...                ...            ...            ...            ...   \n",
       "3474913          0.727          4.456          7.145          3.729   \n",
       "3474914          1.011          4.386          6.950          3.375   \n",
       "3474915          0.642          4.078          8.133          3.436   \n",
       "3474916          0.439          3.965          7.311          3.526   \n",
       "3474917          0.323          4.555          7.871          4.232   \n",
       "\n",
       "         w2mpro-w4mpro  w3mpro-w4mpro  w1-w2-w3  w1-w2-w3-w4     imp_z  \n",
       "0                4.036          1.318     9.170    17.006001  1.031694  \n",
       "1                4.036          1.318     9.170    17.006001  1.033450  \n",
       "2                5.581          3.276    11.535    20.160999  1.210131  \n",
       "3                4.812          3.415    12.213    21.319000  1.213816  \n",
       "4                5.699          3.508    10.849    19.221001  1.659689  \n",
       "...                ...            ...       ...          ...       ...  \n",
       "3474913          6.418          2.689    10.546    19.129999  2.044420  \n",
       "3474914          5.939          2.564    10.290    19.027000  2.857131  \n",
       "3474915          7.491          4.055    11.334    19.254999  1.938109  \n",
       "3474916          6.872          3.346    11.700    20.493000  1.417181  \n",
       "3474917          7.548          3.316    11.788    20.583000  1.000804  \n",
       "\n",
       "[3474918 rows x 107 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8a0cb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: \n",
      " not outlier    3417900\n",
      "outlier          57018\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Flag outlier for improved photo_z\n",
    "df['flag_outlier'] = df_cat_outliers(df, 'z', 'imp_z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f19b5529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels [0,1]:  ['not outlier' 'outlier']\n",
      "Data encoded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "df['flag_outlier'] = le.fit_transform(df['flag_outlier'])\n",
    "print('Labels [0,1]: ',le.inverse_transform([0,1]))\n",
    "print('Data encoded.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a94b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.to_list()\n",
    "features.remove('class')\n",
    "features.remove('z')\n",
    "features.remove('flag_outlier')\n",
    "\n",
    "targets = ['class','flag_outlier']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], \n",
    "                                                    df[targets], \n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle =True, \n",
    "                                                    random_state=0,\n",
    "                                                    stratify=df['flag_outlier'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c591a2d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 12-02 10:08:46] {1926} INFO - task = classification\n",
      "[flaml.automl: 12-02 10:08:46] {1928} INFO - Data split method: stratified\n",
      "[flaml.automl: 12-02 10:08:46] {1932} INFO - Evaluation method: holdout\n",
      "[flaml.automl: 12-02 10:09:03] {1999} INFO - Minimizing error metric: 1-r2\n",
      "[flaml.automl: 12-02 10:09:03] {2051} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
      "[flaml.automl: 12-02 10:09:03] {2291} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl: 12-02 10:09:04] {2405} INFO - Estimated sufficient time budget=1917556s. Estimated necessary time budget=47076s.\n",
      "[flaml.automl: 12-02 10:09:04] {2485} INFO -  at 223.9s,\testimator lgbm's best error=1.0167,\tbest estimator lgbm's best error=1.0167\n",
      "[flaml.automl: 12-02 10:09:04] {2291} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl: 12-02 10:09:05] {2485} INFO -  at 224.3s,\testimator lgbm's best error=1.0167,\tbest estimator lgbm's best error=1.0167\n",
      "[flaml.automl: 12-02 10:09:05] {2291} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl: 12-02 10:09:05] {2485} INFO -  at 224.7s,\testimator lgbm's best error=1.0167,\tbest estimator lgbm's best error=1.0167\n",
      "[flaml.automl: 12-02 10:09:05] {2291} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl: 12-02 10:09:06] {2485} INFO -  at 225.2s,\testimator lgbm's best error=1.0167,\tbest estimator lgbm's best error=1.0167\n",
      "[flaml.automl: 12-02 10:09:06] {2291} INFO - iteration 4, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:06] {2485} INFO -  at 225.6s,\testimator xgboost's best error=1.0167,\tbest estimator lgbm's best error=1.0167\n",
      "[flaml.automl: 12-02 10:09:06] {2291} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:06] {2485} INFO -  at 226.0s,\testimator xgboost's best error=1.0167,\tbest estimator lgbm's best error=1.0167\n",
      "[flaml.automl: 12-02 10:09:06] {2291} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl: 12-02 10:09:07] {2485} INFO -  at 226.4s,\testimator lgbm's best error=1.0167,\tbest estimator lgbm's best error=1.0167\n",
      "[flaml.automl: 12-02 10:09:07] {2291} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl: 12-02 10:09:07] {2485} INFO -  at 226.9s,\testimator lgbm's best error=1.0133,\tbest estimator lgbm's best error=1.0133\n",
      "[flaml.automl: 12-02 10:09:07] {2291} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl: 12-02 10:09:08] {2485} INFO -  at 227.3s,\testimator lgbm's best error=1.0133,\tbest estimator lgbm's best error=1.0133\n",
      "[flaml.automl: 12-02 10:09:08] {2291} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:08] {2485} INFO -  at 227.7s,\testimator xgboost's best error=1.0140,\tbest estimator lgbm's best error=1.0133\n",
      "[flaml.automl: 12-02 10:09:08] {2291} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:08] {2485} INFO -  at 228.1s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:08] {2291} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:09] {2485} INFO -  at 228.6s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:09] {2291} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:09] {2485} INFO -  at 229.0s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:09] {2291} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:10] {2485} INFO -  at 229.5s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:10] {2291} INFO - iteration 14, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:10] {2485} INFO -  at 229.9s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:10] {2291} INFO - iteration 15, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:11] {2485} INFO -  at 230.3s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:11] {2291} INFO - iteration 16, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:11] {2485} INFO -  at 230.8s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:11] {2291} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:12] {2485} INFO -  at 231.3s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:12] {2291} INFO - iteration 18, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:12] {2485} INFO -  at 231.8s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:12] {2291} INFO - iteration 19, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:13] {2485} INFO -  at 232.3s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:13] {2291} INFO - iteration 20, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:13] {2485} INFO -  at 232.7s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:13] {2291} INFO - iteration 21, current learner rf\n",
      "[flaml.automl: 12-02 10:09:14] {2485} INFO -  at 233.3s,\testimator rf's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:14] {2291} INFO - iteration 22, current learner rf\n",
      "[flaml.automl: 12-02 10:09:15] {2485} INFO -  at 234.6s,\testimator rf's best error=1.0158,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:15] {2291} INFO - iteration 23, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:15] {2485} INFO -  at 235.1s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:15] {2291} INFO - iteration 24, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:16] {2485} INFO -  at 235.8s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:16] {2291} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:17] {2485} INFO -  at 236.2s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:17] {2291} INFO - iteration 26, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:17] {2485} INFO -  at 236.8s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:17] {2291} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:18] {2485} INFO -  at 237.3s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:18] {2291} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:18] {2485} INFO -  at 237.8s,\testimator extra_tree's best error=1.0167,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:18] {2291} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:09:19] {2485} INFO -  at 238.3s,\testimator extra_tree's best error=1.0149,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:19] {2291} INFO - iteration 30, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:19] {2485} INFO -  at 238.8s,\testimator xgboost's best error=0.9839,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:19] {2291} INFO - iteration 31, current learner rf\n",
      "[flaml.automl: 12-02 10:09:20] {2485} INFO -  at 239.9s,\testimator rf's best error=1.0158,\tbest estimator xgboost's best error=0.9839\n",
      "[flaml.automl: 12-02 10:09:20] {2291} INFO - iteration 32, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:21] {2485} INFO -  at 240.4s,\testimator xgboost's best error=0.9489,\tbest estimator xgboost's best error=0.9489\n",
      "[flaml.automl: 12-02 10:09:21] {2291} INFO - iteration 33, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:21] {2485} INFO -  at 241.0s,\testimator xgboost's best error=0.9489,\tbest estimator xgboost's best error=0.9489\n",
      "[flaml.automl: 12-02 10:09:21] {2291} INFO - iteration 34, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:22] {2485} INFO -  at 241.6s,\testimator xgboost's best error=0.9489,\tbest estimator xgboost's best error=0.9489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 12-02 10:09:22] {2291} INFO - iteration 35, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:23] {2485} INFO -  at 242.2s,\testimator xgboost's best error=0.9489,\tbest estimator xgboost's best error=0.9489\n",
      "[flaml.automl: 12-02 10:09:23] {2291} INFO - iteration 36, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:23] {2485} INFO -  at 242.8s,\testimator xgboost's best error=0.9489,\tbest estimator xgboost's best error=0.9489\n",
      "[flaml.automl: 12-02 10:09:23] {2291} INFO - iteration 37, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:24] {2485} INFO -  at 244.1s,\testimator xgboost's best error=0.9489,\tbest estimator xgboost's best error=0.9489\n",
      "[flaml.automl: 12-02 10:09:24] {2291} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:26] {2485} INFO -  at 245.3s,\testimator xgboost's best error=0.9489,\tbest estimator xgboost's best error=0.9489\n",
      "[flaml.automl: 12-02 10:09:26] {2291} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:27] {2485} INFO -  at 246.9s,\testimator xgboost's best error=0.9117,\tbest estimator xgboost's best error=0.9117\n",
      "[flaml.automl: 12-02 10:09:27] {2291} INFO - iteration 40, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:28] {2485} INFO -  at 248.1s,\testimator xgboost's best error=0.9117,\tbest estimator xgboost's best error=0.9117\n",
      "[flaml.automl: 12-02 10:09:28] {2291} INFO - iteration 41, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:32] {2485} INFO -  at 252.1s,\testimator xgboost's best error=0.8255,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:09:32] {2291} INFO - iteration 42, current learner rf\n",
      "[flaml.automl: 12-02 10:09:33] {2485} INFO -  at 253.1s,\testimator rf's best error=1.0049,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:09:33] {2291} INFO - iteration 43, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:36] {2485} INFO -  at 256.0s,\testimator xgboost's best error=0.8255,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:09:36] {2291} INFO - iteration 44, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:43] {2485} INFO -  at 262.2s,\testimator xgboost's best error=0.8255,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:09:43] {2291} INFO - iteration 45, current learner xgboost\n",
      "[flaml.automl: 12-02 10:09:46] {2485} INFO -  at 265.8s,\testimator xgboost's best error=0.8255,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:09:46] {2291} INFO - iteration 46, current learner catboost\n",
      "[flaml.automl: 12-02 10:09:49] {2485} INFO -  at 268.4s,\testimator catboost's best error=1.0084,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:09:49] {2291} INFO - iteration 47, current learner catboost\n",
      "[flaml.automl: 12-02 10:09:57] {2485} INFO -  at 276.7s,\testimator catboost's best error=1.0084,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:09:57] {2291} INFO - iteration 48, current learner xgboost\n",
      "[flaml.automl: 12-02 10:10:03] {2485} INFO -  at 282.8s,\testimator xgboost's best error=0.8255,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:10:03] {2291} INFO - iteration 49, current learner catboost\n",
      "[flaml.automl: 12-02 10:10:06] {2485} INFO -  at 285.2s,\testimator catboost's best error=1.0033,\tbest estimator xgboost's best error=0.8255\n",
      "[flaml.automl: 12-02 10:10:06] {2291} INFO - iteration 50, current learner xgboost\n",
      "[flaml.automl: 12-02 10:10:30] {2485} INFO -  at 309.4s,\testimator xgboost's best error=0.7664,\tbest estimator xgboost's best error=0.7664\n",
      "[flaml.automl: 12-02 10:10:30] {2291} INFO - iteration 51, current learner xgboost\n",
      "[flaml.automl: 12-02 10:10:44] {2485} INFO -  at 323.6s,\testimator xgboost's best error=0.7664,\tbest estimator xgboost's best error=0.7664\n",
      "[flaml.automl: 12-02 10:10:44] {2291} INFO - iteration 52, current learner rf\n",
      "[flaml.automl: 12-02 10:10:45] {2485} INFO -  at 324.5s,\testimator rf's best error=1.0049,\tbest estimator xgboost's best error=0.7664\n",
      "[flaml.automl: 12-02 10:10:45] {2291} INFO - iteration 53, current learner rf\n",
      "[flaml.automl: 12-02 10:10:46] {2485} INFO -  at 325.5s,\testimator rf's best error=1.0049,\tbest estimator xgboost's best error=0.7664\n",
      "[flaml.automl: 12-02 10:10:46] {2291} INFO - iteration 54, current learner rf\n",
      "[flaml.automl: 12-02 10:10:47] {2485} INFO -  at 326.5s,\testimator rf's best error=1.0049,\tbest estimator xgboost's best error=0.7664\n",
      "[flaml.automl: 12-02 10:10:47] {2291} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl: 12-02 10:10:47] {2485} INFO -  at 326.9s,\testimator lgbm's best error=1.0133,\tbest estimator xgboost's best error=0.7664\n",
      "[flaml.automl: 12-02 10:10:47] {2291} INFO - iteration 56, current learner xgboost\n",
      "[flaml.automl: 12-02 10:12:14] {2485} INFO -  at 413.5s,\testimator xgboost's best error=0.6969,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:14] {2291} INFO - iteration 57, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:12:15] {2485} INFO -  at 414.2s,\testimator xgb_limitdepth's best error=1.0256,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:15] {2291} INFO - iteration 58, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:12:15] {2485} INFO -  at 414.7s,\testimator xgb_limitdepth's best error=1.0026,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:15] {2291} INFO - iteration 59, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:12:16] {2485} INFO -  at 415.4s,\testimator xgb_limitdepth's best error=1.0026,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:16] {2291} INFO - iteration 60, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:12:16] {2485} INFO -  at 415.9s,\testimator xgb_limitdepth's best error=1.0026,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:16] {2291} INFO - iteration 61, current learner rf\n",
      "[flaml.automl: 12-02 10:12:17] {2485} INFO -  at 416.8s,\testimator rf's best error=1.0049,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:17] {2291} INFO - iteration 62, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:12:18] {2485} INFO -  at 417.4s,\testimator xgb_limitdepth's best error=1.0026,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:18] {2291} INFO - iteration 63, current learner rf\n",
      "[flaml.automl: 12-02 10:12:18] {2485} INFO -  at 418.0s,\testimator rf's best error=1.0049,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:18] {2291} INFO - iteration 64, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:12:19] {2485} INFO -  at 418.4s,\testimator xgb_limitdepth's best error=1.0026,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:19] {2291} INFO - iteration 65, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:12:20] {2485} INFO -  at 419.6s,\testimator xgb_limitdepth's best error=0.9819,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:20] {2291} INFO - iteration 66, current learner rf\n",
      "[flaml.automl: 12-02 10:12:21] {2485} INFO -  at 421.1s,\testimator rf's best error=0.9977,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:12:21] {2291} INFO - iteration 67, current learner xgboost\n",
      "[flaml.automl: 12-02 10:13:14] {2485} INFO -  at 473.7s,\testimator xgboost's best error=0.6969,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:13:14] {2291} INFO - iteration 68, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:13:15] {2485} INFO -  at 475.1s,\testimator xgb_limitdepth's best error=0.9307,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:13:15] {2291} INFO - iteration 69, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:13:17] {2485} INFO -  at 476.2s,\testimator xgb_limitdepth's best error=0.9307,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:13:17] {2291} INFO - iteration 70, current learner rf\n",
      "[flaml.automl: 12-02 10:13:18] {2485} INFO -  at 477.8s,\testimator rf's best error=0.9977,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:13:18] {2291} INFO - iteration 71, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:13:20] {2485} INFO -  at 479.3s,\testimator xgb_limitdepth's best error=0.9307,\tbest estimator xgboost's best error=0.6969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 12-02 10:13:20] {2291} INFO - iteration 72, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:13:21] {2485} INFO -  at 480.4s,\testimator xgb_limitdepth's best error=0.9307,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:13:21] {2291} INFO - iteration 73, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:13:22] {2485} INFO -  at 481.3s,\testimator xgb_limitdepth's best error=0.9307,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:13:22] {2291} INFO - iteration 74, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:13:24] {2485} INFO -  at 483.3s,\testimator xgb_limitdepth's best error=0.9298,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:13:24] {2291} INFO - iteration 75, current learner xgboost\n",
      "[flaml.automl: 12-02 10:15:20] {2485} INFO -  at 600.0s,\testimator xgboost's best error=0.6969,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:20] {2291} INFO - iteration 76, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:21] {2485} INFO -  at 600.2s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:21] {2291} INFO - iteration 77, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:21] {2485} INFO -  at 600.5s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:21] {2291} INFO - iteration 78, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:21] {2485} INFO -  at 600.7s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:21] {2291} INFO - iteration 79, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:22] {2485} INFO -  at 601.6s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:22] {2291} INFO - iteration 80, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:23] {2485} INFO -  at 602.7s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:23] {2291} INFO - iteration 81, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:24] {2485} INFO -  at 603.7s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:24] {2291} INFO - iteration 82, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:29] {2485} INFO -  at 608.2s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:29] {2291} INFO - iteration 83, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:32] {2485} INFO -  at 612.1s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:32] {2291} INFO - iteration 84, current learner lrl1\n",
      "[flaml.automl: 12-02 10:15:36] {2485} INFO -  at 615.2s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:15:36] {2291} INFO - iteration 85, current learner lrl1\n",
      "[flaml.automl: 12-02 10:16:31] {2485} INFO -  at 670.7s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:16:31] {2291} INFO - iteration 86, current learner lrl1\n",
      "[flaml.automl: 12-02 10:17:26] {2485} INFO -  at 725.8s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:17:26] {2291} INFO - iteration 87, current learner lrl1\n",
      "[flaml.automl: 12-02 10:18:27] {2485} INFO -  at 786.2s,\testimator lrl1's best error=1.0167,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:18:27] {2291} INFO - iteration 88, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:18:27] {2485} INFO -  at 786.8s,\testimator extra_tree's best error=1.0149,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:18:27] {2291} INFO - iteration 89, current learner lrl1\n",
      "[flaml.automl: 12-02 10:18:27] {2511} INFO - stop trying learner lrl1\n",
      "[flaml.automl: 12-02 10:18:27] {2291} INFO - iteration 90, current learner catboost\n",
      "[flaml.automl: 12-02 10:18:35] {2485} INFO -  at 794.5s,\testimator catboost's best error=1.0033,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:18:35] {2291} INFO - iteration 91, current learner rf\n",
      "[flaml.automl: 12-02 10:18:41] {2485} INFO -  at 800.2s,\testimator rf's best error=0.9516,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:18:41] {2291} INFO - iteration 92, current learner xgboost\n",
      "[flaml.automl: 12-02 10:19:05] {2485} INFO -  at 824.5s,\testimator xgboost's best error=0.6969,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:05] {2291} INFO - iteration 93, current learner rf\n",
      "[flaml.automl: 12-02 10:19:06] {2485} INFO -  at 826.1s,\testimator rf's best error=0.9516,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:06] {2291} INFO - iteration 94, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:19:07] {2485} INFO -  at 827.0s,\testimator extra_tree's best error=1.0044,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:07] {2291} INFO - iteration 95, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:19:08] {2485} INFO -  at 827.4s,\testimator extra_tree's best error=0.9906,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:08] {2291} INFO - iteration 96, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:19:09] {2485} INFO -  at 828.3s,\testimator extra_tree's best error=0.9906,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:09] {2291} INFO - iteration 97, current learner rf\n",
      "[flaml.automl: 12-02 10:19:13] {2485} INFO -  at 832.9s,\testimator rf's best error=0.9483,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:13] {2291} INFO - iteration 98, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:19:14] {2485} INFO -  at 833.5s,\testimator extra_tree's best error=0.9906,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:14] {2291} INFO - iteration 99, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:19:14] {2485} INFO -  at 834.0s,\testimator extra_tree's best error=0.9906,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:14] {2291} INFO - iteration 100, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:19:16] {2485} INFO -  at 835.6s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:16] {2291} INFO - iteration 101, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:19:17] {2485} INFO -  at 836.8s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6969\n",
      "[flaml.automl: 12-02 10:19:17] {2291} INFO - iteration 102, current learner xgboost\n",
      "[flaml.automl: 12-02 10:26:36] {2485} INFO -  at 1276.0s,\testimator xgboost's best error=0.6864,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:26:36] {2291} INFO - iteration 103, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:26:39] {2485} INFO -  at 1278.2s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:26:39] {2291} INFO - iteration 104, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:26:40] {2485} INFO -  at 1279.9s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:26:40] {2291} INFO - iteration 105, current learner lgbm\n",
      "[flaml.automl: 12-02 10:26:41] {2485} INFO -  at 1280.3s,\testimator lgbm's best error=1.0133,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:26:41] {2291} INFO - iteration 106, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:26:42] {2485} INFO -  at 1281.7s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:26:42] {2291} INFO - iteration 107, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:27:14] {2485} INFO -  at 1313.2s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:27:14] {2291} INFO - iteration 108, current learner xgboost\n",
      "[flaml.automl: 12-02 10:33:02] {2485} INFO -  at 1661.2s,\testimator xgboost's best error=0.6864,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:33:02] {2291} INFO - iteration 109, current learner extra_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 12-02 10:33:57] {2485} INFO -  at 1716.7s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:33:57] {2291} INFO - iteration 110, current learner rf\n",
      "[flaml.automl: 12-02 10:34:02] {2485} INFO -  at 1722.1s,\testimator rf's best error=0.9483,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:34:02] {2291} INFO - iteration 111, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:34:52] {2485} INFO -  at 1771.7s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:34:52] {2291} INFO - iteration 112, current learner xgboost\n",
      "[flaml.automl: 12-02 10:39:59] {2485} INFO -  at 2078.2s,\testimator xgboost's best error=0.6864,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:39:59] {2291} INFO - iteration 113, current learner lgbm\n",
      "[flaml.automl: 12-02 10:39:59] {2485} INFO -  at 2078.4s,\testimator lgbm's best error=1.0133,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:39:59] {2291} INFO - iteration 114, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:41:16] {2485} INFO -  at 2156.1s,\testimator extra_tree's best error=0.9367,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:41:16] {2291} INFO - iteration 115, current learner xgboost\n",
      "[flaml.automl: 12-02 10:52:00] {2485} INFO -  at 2799.3s,\testimator xgboost's best error=0.6864,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:52:00] {2291} INFO - iteration 116, current learner xgb_limitdepth\n",
      "[flaml.automl: 12-02 10:52:01] {2485} INFO -  at 2800.3s,\testimator xgb_limitdepth's best error=0.9298,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:52:01] {2291} INFO - iteration 117, current learner lgbm\n",
      "[flaml.automl: 12-02 10:52:01] {2485} INFO -  at 2800.6s,\testimator lgbm's best error=1.0100,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:52:01] {2291} INFO - iteration 118, current learner lgbm\n",
      "[flaml.automl: 12-02 10:52:01] {2485} INFO -  at 2801.0s,\testimator lgbm's best error=1.0100,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:52:01] {2291} INFO - iteration 119, current learner extra_tree\n",
      "[flaml.automl: 12-02 10:52:37] {2485} INFO -  at 2836.6s,\testimator extra_tree's best error=0.9235,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:52:37] {2291} INFO - iteration 120, current learner catboost\n",
      "[flaml.automl: 12-02 10:52:40] {2485} INFO -  at 2839.2s,\testimator catboost's best error=1.0033,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:52:40] {2291} INFO - iteration 121, current learner lgbm\n",
      "[flaml.automl: 12-02 10:52:40] {2485} INFO -  at 2839.5s,\testimator lgbm's best error=1.0100,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:52:40] {2291} INFO - iteration 122, current learner xgboost\n",
      "[flaml.automl: 12-02 10:56:12] {2485} INFO -  at 3051.7s,\testimator xgboost's best error=0.6864,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 10:56:12] {2291} INFO - iteration 123, current learner xgboost\n",
      "[flaml.automl: 12-02 11:15:26] {2485} INFO -  at 4205.9s,\testimator xgboost's best error=0.6864,\tbest estimator xgboost's best error=0.6864\n",
      "[flaml.automl: 12-02 11:23:10] {2697} INFO - retrain xgboost for 463.3s\n",
      "[flaml.automl: 12-02 11:23:10] {2702} INFO - retrained model: XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "              colsample_bylevel=0.9581891971407994, colsample_bynode=1,\n",
      "              colsample_bytree=0.8717346660187126, gamma=0, gpu_id=-1,\n",
      "              grow_policy='lossguide', importance_type='gain',\n",
      "              interaction_constraints='', learning_rate=0.04154046124800973,\n",
      "              max_delta_step=0, max_depth=0, max_leaves=287,\n",
      "              min_child_weight=40.1443074313098, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=777, n_jobs=-1,\n",
      "              num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0.0016352264865901315, reg_lambda=0.12869217985375717,\n",
      "              scale_pos_weight=1, subsample=0.9325030188198649,\n",
      "              tree_method='hist', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=0)\n",
      "[flaml.automl: 12-02 11:23:10] {2080} INFO - fit succeeded\n",
      "[flaml.automl: 12-02 11:23:10] {2082} INFO - Time taken to find the best model: 1276.0390729904175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: {'lgbm': {'n_estimators': 18, 'num_leaves': 25, 'min_child_samples': 10, 'learning_rate': 0.043389227182311904, 'log_max_bin': 7, 'colsample_bytree': 0.8057683870789988, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04635370841900571, 'FLAML_sample_size': 10000}, 'rf': {'n_estimators': 4, 'max_features': 0.7006670149394767, 'max_leaves': 88, 'criterion': 'entropy', 'FLAML_sample_size': 40000}, 'catboost': {'early_stopping_rounds': 11, 'learning_rate': 0.2, 'n_estimators': 74, 'FLAML_sample_size': 10000}, 'xgboost': {'n_estimators': 777, 'max_leaves': 287, 'min_child_weight': 40.1443074313098, 'learning_rate': 0.04154046124800973, 'subsample': 0.9325030188198649, 'colsample_bylevel': 0.9581891971407994, 'colsample_bytree': 0.8717346660187126, 'reg_alpha': 0.0016352264865901315, 'reg_lambda': 0.12869217985375717, 'FLAML_sample_size': 2501940}, 'extra_tree': {'n_estimators': 4, 'max_features': 1.0, 'max_leaves': 84, 'criterion': 'gini', 'FLAML_sample_size': 2501940}, 'xgb_limitdepth': {'n_estimators': 37, 'max_depth': 6, 'min_child_weight': 39.80364574649833, 'learning_rate': 0.19065088271829803, 'subsample': 1.0, 'colsample_bylevel': 0.739977180478103, 'colsample_bytree': 0.7289333413729882, 'reg_alpha': 0.0009765625, 'reg_lambda': 8.545600467621691, 'FLAML_sample_size': 40000}, 'lrl1': {'C': 1.0, 'FLAML_sample_size': 10000}}\n"
     ]
    }
   ],
   "source": [
    "automl_clf = flaml_opt(['all'], X_train, y_train['flag_outlier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e391443",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_clf = xgb.XGBClassifier(base_score=0.5, booster='gbtree',\n",
    "              colsample_bylevel=0.9581891971407994, colsample_bynode=1,\n",
    "              colsample_bytree=0.8717346660187126, gamma=0, gpu_id=-1,\n",
    "              grow_policy='lossguide', importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.04154046124800973,\n",
    "              max_delta_step=0, max_depth=0, max_leaves=287,\n",
    "              min_child_weight=40.1443074313098, missing=-9999.0,\n",
    "              monotone_constraints='()', n_estimators=777, n_jobs=-1,\n",
    "              num_parallel_tree=1, random_state=0,\n",
    "              reg_alpha=0.0016352264865901315, reg_lambda=0.12869217985375717,\n",
    "              scale_pos_weight=1, subsample=0.9325030188198649,\n",
    "              tree_method='gpu_hist', use_label_encoder=False,\n",
    "              validate_parameters=1, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009cbc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cycle loop: 1\n",
      "For cycle loop: 2\n",
      "For cycle loop: 3\n",
      "For cycle loop: 4\n",
      "For cycle loop: 5\n",
      "For cycle loop: 6\n",
      "For cycle loop: 7\n",
      "For cycle loop: 8\n",
      "For cycle loop: 9\n",
      "For cycle loop: 10\n",
      "For cycle loop: 11\n",
      "For cycle loop: 12\n",
      "For cycle loop: 13\n",
      "For cycle loop: 14\n",
      "For cycle loop: 15\n",
      "For cycle loop: 16\n",
      "For cycle loop: 17\n",
      "For cycle loop: 18\n",
      "For cycle loop: 19\n",
      "For cycle loop: 20\n",
      "For cycle loop: 21\n",
      "For cycle loop: 22\n",
      "For cycle loop: 23\n",
      "For cycle loop: 24\n",
      "For cycle loop: 25\n",
      "For cycle loop: 26\n",
      "For cycle loop: 27\n",
      "For cycle loop: 28\n",
      "For cycle loop: 29\n",
      "For cycle loop: 30\n",
      "For cycle loop: 31\n",
      "For cycle loop: 32\n",
      "For cycle loop: 33\n",
      "For cycle loop: 34\n",
      "For cycle loop: 35\n",
      "For cycle loop: 36\n",
      "For cycle loop: 37\n",
      "For cycle loop: 38\n",
      "For cycle loop: 39\n",
      "For cycle loop: 40\n",
      "For cycle loop: 41\n",
      "For cycle loop: 42\n",
      "For cycle loop: 43\n",
      "For cycle loop: 44\n",
      "For cycle loop: 45\n",
      "For cycle loop: 46\n",
      "For cycle loop: 47\n",
      "For cycle loop: 48\n",
      "For cycle loop: 49\n",
      "For cycle loop: 50\n"
     ]
    }
   ],
   "source": [
    "oof_train, oof_test = kf_oof(automl_clf,X_train,y_train,X_test,'flag_outlier',nkf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bb19c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train = pd.Series(oof_train, index=X_train.index)\n",
    "oof_test = pd.Series(np.round(oof_test), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11c61281",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF_train: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99059   0.99788   0.99422   2734320\n",
      "           1    0.77276   0.43158   0.55384     45614\n",
      "\n",
      "    accuracy                        0.98859   2779934\n",
      "   macro avg    0.88167   0.71473   0.77403   2779934\n",
      "weighted avg    0.98701   0.98859   0.98700   2779934\n",
      "\n",
      "\n",
      " Confusion matrix: \n",
      " [[2728531    5789]\n",
      " [  25928   19686]]\n",
      "OOF_test: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99073   0.99813   0.99442    683580\n",
      "           1    0.79740   0.44037   0.56739     11404\n",
      "\n",
      "    accuracy                        0.98898    694984\n",
      "   macro avg    0.89406   0.71925   0.78091    694984\n",
      "weighted avg    0.98756   0.98898   0.98741    694984\n",
      "\n",
      "\n",
      " Confusion matrix: \n",
      " [[682304   1276]\n",
      " [  6382   5022]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import  confusion_matrix, classification_report\n",
    "\n",
    "print('OOF_train: \\n ', classification_report(y_train['flag_outlier'], oof_train, digits=5))\n",
    "print('\\n Confusion matrix: \\n', confusion_matrix(y_train['flag_outlier'], oof_train))\n",
    "print('OOF_test: \\n ', classification_report(y_test['flag_outlier'], oof_test, digits=5))\n",
    "print('\\n Confusion matrix: \\n', confusion_matrix(y_test['flag_outlier'], oof_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eacf935",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_flag = pd.concat([oof_train, oof_test]).sort_index()\n",
    "oof_flag.name = 'oof_flag_outlier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ff4d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,oof_flag, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "731da75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['oof_flag_outlier'] = pd.to_numeric(df['oof_flag_outlier'], downcast=\"float\")\n",
    "df['flag_outlier'] = pd.to_numeric(df['flag_outlier'], downcast=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd604aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specObjID</th>\n",
       "      <th>psfMag_u</th>\n",
       "      <th>psfMag_g</th>\n",
       "      <th>psfMag_r</th>\n",
       "      <th>psfMag_i</th>\n",
       "      <th>psfMag_z</th>\n",
       "      <th>w1mpro</th>\n",
       "      <th>w2mpro</th>\n",
       "      <th>w3mpro</th>\n",
       "      <th>w4mpro</th>\n",
       "      <th>...</th>\n",
       "      <th>w1mpro-w3mpro</th>\n",
       "      <th>w1mpro-w4mpro</th>\n",
       "      <th>w2mpro-w3mpro</th>\n",
       "      <th>w2mpro-w4mpro</th>\n",
       "      <th>w3mpro-w4mpro</th>\n",
       "      <th>w1-w2-w3</th>\n",
       "      <th>w1-w2-w3-w4</th>\n",
       "      <th>imp_z</th>\n",
       "      <th>flag_outlier</th>\n",
       "      <th>oof_flag_outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299489676975171584</td>\n",
       "      <td>19.086210</td>\n",
       "      <td>17.417789</td>\n",
       "      <td>16.595810</td>\n",
       "      <td>16.034969</td>\n",
       "      <td>15.680770</td>\n",
       "      <td>11.856000</td>\n",
       "      <td>11.872</td>\n",
       "      <td>9.154</td>\n",
       "      <td>7.836</td>\n",
       "      <td>...</td>\n",
       "      <td>2.702</td>\n",
       "      <td>4.020</td>\n",
       "      <td>2.718</td>\n",
       "      <td>4.036</td>\n",
       "      <td>1.318</td>\n",
       "      <td>9.170</td>\n",
       "      <td>17.006001</td>\n",
       "      <td>1.031694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>299489677444933632</td>\n",
       "      <td>19.086210</td>\n",
       "      <td>17.417789</td>\n",
       "      <td>16.595810</td>\n",
       "      <td>16.034969</td>\n",
       "      <td>15.680770</td>\n",
       "      <td>11.856000</td>\n",
       "      <td>11.872</td>\n",
       "      <td>9.154</td>\n",
       "      <td>7.836</td>\n",
       "      <td>...</td>\n",
       "      <td>2.702</td>\n",
       "      <td>4.020</td>\n",
       "      <td>2.718</td>\n",
       "      <td>4.036</td>\n",
       "      <td>1.318</td>\n",
       "      <td>9.170</td>\n",
       "      <td>17.006001</td>\n",
       "      <td>1.033450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>299489951853078528</td>\n",
       "      <td>22.678499</td>\n",
       "      <td>20.570419</td>\n",
       "      <td>19.164761</td>\n",
       "      <td>18.745230</td>\n",
       "      <td>18.312450</td>\n",
       "      <td>14.574000</td>\n",
       "      <td>14.207</td>\n",
       "      <td>11.902</td>\n",
       "      <td>8.626</td>\n",
       "      <td>...</td>\n",
       "      <td>2.672</td>\n",
       "      <td>5.948</td>\n",
       "      <td>2.305</td>\n",
       "      <td>5.581</td>\n",
       "      <td>3.276</td>\n",
       "      <td>11.535</td>\n",
       "      <td>20.160999</td>\n",
       "      <td>1.210131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299489952322840576</td>\n",
       "      <td>22.343210</td>\n",
       "      <td>20.316629</td>\n",
       "      <td>18.933720</td>\n",
       "      <td>18.372570</td>\n",
       "      <td>17.988461</td>\n",
       "      <td>14.226000</td>\n",
       "      <td>13.918</td>\n",
       "      <td>12.521</td>\n",
       "      <td>9.106</td>\n",
       "      <td>...</td>\n",
       "      <td>1.705</td>\n",
       "      <td>5.120</td>\n",
       "      <td>1.397</td>\n",
       "      <td>4.812</td>\n",
       "      <td>3.415</td>\n",
       "      <td>12.213</td>\n",
       "      <td>21.319000</td>\n",
       "      <td>1.213816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>299490226730985472</td>\n",
       "      <td>20.024401</td>\n",
       "      <td>19.601589</td>\n",
       "      <td>19.492901</td>\n",
       "      <td>19.371880</td>\n",
       "      <td>19.298290</td>\n",
       "      <td>15.102000</td>\n",
       "      <td>14.071</td>\n",
       "      <td>11.880</td>\n",
       "      <td>8.372</td>\n",
       "      <td>...</td>\n",
       "      <td>3.222</td>\n",
       "      <td>6.730</td>\n",
       "      <td>2.191</td>\n",
       "      <td>5.699</td>\n",
       "      <td>3.508</td>\n",
       "      <td>10.849</td>\n",
       "      <td>19.221001</td>\n",
       "      <td>1.659689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474913</th>\n",
       "      <td>11259270496166846464</td>\n",
       "      <td>23.912531</td>\n",
       "      <td>23.555460</td>\n",
       "      <td>22.746799</td>\n",
       "      <td>21.878040</td>\n",
       "      <td>20.049231</td>\n",
       "      <td>15.729000</td>\n",
       "      <td>15.002</td>\n",
       "      <td>11.273</td>\n",
       "      <td>8.584</td>\n",
       "      <td>...</td>\n",
       "      <td>4.456</td>\n",
       "      <td>7.145</td>\n",
       "      <td>3.729</td>\n",
       "      <td>6.418</td>\n",
       "      <td>2.689</td>\n",
       "      <td>10.546</td>\n",
       "      <td>19.129999</td>\n",
       "      <td>2.044420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474914</th>\n",
       "      <td>11259270771044753408</td>\n",
       "      <td>19.719530</td>\n",
       "      <td>19.538170</td>\n",
       "      <td>19.356890</td>\n",
       "      <td>18.952471</td>\n",
       "      <td>18.992161</td>\n",
       "      <td>15.687000</td>\n",
       "      <td>14.676</td>\n",
       "      <td>11.301</td>\n",
       "      <td>8.737</td>\n",
       "      <td>...</td>\n",
       "      <td>4.386</td>\n",
       "      <td>6.950</td>\n",
       "      <td>3.375</td>\n",
       "      <td>5.939</td>\n",
       "      <td>2.564</td>\n",
       "      <td>10.290</td>\n",
       "      <td>19.027000</td>\n",
       "      <td>2.857131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474915</th>\n",
       "      <td>11259271045922660352</td>\n",
       "      <td>26.007179</td>\n",
       "      <td>23.568859</td>\n",
       "      <td>22.621071</td>\n",
       "      <td>21.113041</td>\n",
       "      <td>20.064899</td>\n",
       "      <td>16.054001</td>\n",
       "      <td>15.412</td>\n",
       "      <td>11.976</td>\n",
       "      <td>7.921</td>\n",
       "      <td>...</td>\n",
       "      <td>4.078</td>\n",
       "      <td>8.133</td>\n",
       "      <td>3.436</td>\n",
       "      <td>7.491</td>\n",
       "      <td>4.055</td>\n",
       "      <td>11.334</td>\n",
       "      <td>19.254999</td>\n",
       "      <td>1.938109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474916</th>\n",
       "      <td>11259271320800567296</td>\n",
       "      <td>24.128031</td>\n",
       "      <td>22.385111</td>\n",
       "      <td>21.015579</td>\n",
       "      <td>20.358210</td>\n",
       "      <td>19.912359</td>\n",
       "      <td>16.104000</td>\n",
       "      <td>15.665</td>\n",
       "      <td>12.139</td>\n",
       "      <td>8.793</td>\n",
       "      <td>...</td>\n",
       "      <td>3.965</td>\n",
       "      <td>7.311</td>\n",
       "      <td>3.526</td>\n",
       "      <td>6.872</td>\n",
       "      <td>3.346</td>\n",
       "      <td>11.700</td>\n",
       "      <td>20.493000</td>\n",
       "      <td>1.417181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474917</th>\n",
       "      <td>11259274069579636736</td>\n",
       "      <td>23.662201</td>\n",
       "      <td>22.239361</td>\n",
       "      <td>20.752819</td>\n",
       "      <td>19.511320</td>\n",
       "      <td>18.673740</td>\n",
       "      <td>16.666000</td>\n",
       "      <td>16.343</td>\n",
       "      <td>12.111</td>\n",
       "      <td>8.795</td>\n",
       "      <td>...</td>\n",
       "      <td>4.555</td>\n",
       "      <td>7.871</td>\n",
       "      <td>4.232</td>\n",
       "      <td>7.548</td>\n",
       "      <td>3.316</td>\n",
       "      <td>11.788</td>\n",
       "      <td>20.583000</td>\n",
       "      <td>1.000804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3474918 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    specObjID   psfMag_u   psfMag_g   psfMag_r   psfMag_i  \\\n",
       "0          299489676975171584  19.086210  17.417789  16.595810  16.034969   \n",
       "1          299489677444933632  19.086210  17.417789  16.595810  16.034969   \n",
       "2          299489951853078528  22.678499  20.570419  19.164761  18.745230   \n",
       "3          299489952322840576  22.343210  20.316629  18.933720  18.372570   \n",
       "4          299490226730985472  20.024401  19.601589  19.492901  19.371880   \n",
       "...                       ...        ...        ...        ...        ...   \n",
       "3474913  11259270496166846464  23.912531  23.555460  22.746799  21.878040   \n",
       "3474914  11259270771044753408  19.719530  19.538170  19.356890  18.952471   \n",
       "3474915  11259271045922660352  26.007179  23.568859  22.621071  21.113041   \n",
       "3474916  11259271320800567296  24.128031  22.385111  21.015579  20.358210   \n",
       "3474917  11259274069579636736  23.662201  22.239361  20.752819  19.511320   \n",
       "\n",
       "          psfMag_z     w1mpro  w2mpro  w3mpro  w4mpro  ...  w1mpro-w3mpro  \\\n",
       "0        15.680770  11.856000  11.872   9.154   7.836  ...          2.702   \n",
       "1        15.680770  11.856000  11.872   9.154   7.836  ...          2.702   \n",
       "2        18.312450  14.574000  14.207  11.902   8.626  ...          2.672   \n",
       "3        17.988461  14.226000  13.918  12.521   9.106  ...          1.705   \n",
       "4        19.298290  15.102000  14.071  11.880   8.372  ...          3.222   \n",
       "...            ...        ...     ...     ...     ...  ...            ...   \n",
       "3474913  20.049231  15.729000  15.002  11.273   8.584  ...          4.456   \n",
       "3474914  18.992161  15.687000  14.676  11.301   8.737  ...          4.386   \n",
       "3474915  20.064899  16.054001  15.412  11.976   7.921  ...          4.078   \n",
       "3474916  19.912359  16.104000  15.665  12.139   8.793  ...          3.965   \n",
       "3474917  18.673740  16.666000  16.343  12.111   8.795  ...          4.555   \n",
       "\n",
       "         w1mpro-w4mpro  w2mpro-w3mpro  w2mpro-w4mpro  w3mpro-w4mpro  w1-w2-w3  \\\n",
       "0                4.020          2.718          4.036          1.318     9.170   \n",
       "1                4.020          2.718          4.036          1.318     9.170   \n",
       "2                5.948          2.305          5.581          3.276    11.535   \n",
       "3                5.120          1.397          4.812          3.415    12.213   \n",
       "4                6.730          2.191          5.699          3.508    10.849   \n",
       "...                ...            ...            ...            ...       ...   \n",
       "3474913          7.145          3.729          6.418          2.689    10.546   \n",
       "3474914          6.950          3.375          5.939          2.564    10.290   \n",
       "3474915          8.133          3.436          7.491          4.055    11.334   \n",
       "3474916          7.311          3.526          6.872          3.346    11.700   \n",
       "3474917          7.871          4.232          7.548          3.316    11.788   \n",
       "\n",
       "         w1-w2-w3-w4     imp_z  flag_outlier  oof_flag_outlier  \n",
       "0          17.006001  1.031694           0.0               0.0  \n",
       "1          17.006001  1.033450           0.0               0.0  \n",
       "2          20.160999  1.210131           0.0               0.0  \n",
       "3          21.319000  1.213816           0.0               0.0  \n",
       "4          19.221001  1.659689           0.0               0.0  \n",
       "...              ...       ...           ...               ...  \n",
       "3474913    19.129999  2.044420           0.0               0.0  \n",
       "3474914    19.027000  2.857131           0.0               0.0  \n",
       "3474915    19.254999  1.938109           0.0               0.0  \n",
       "3474916    20.493000  1.417181           0.0               0.0  \n",
       "3474917    20.583000  1.000804           0.0               0.0  \n",
       "\n",
       "[3474918 rows x 109 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff31022",
   "metadata": {},
   "source": [
    "# Export DataFrame in pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b12967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"./Data/SDSS_DR15_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dc80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
